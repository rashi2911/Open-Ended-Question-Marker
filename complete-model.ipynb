{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0903033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\requests\\__init__.py:78: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({0}) or chardet ({1}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from math import *\n",
    "import time\n",
    "import en_core_web_lg\n",
    "from rake_nltk import Rake\n",
    "import pytextrank\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150ccfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopw = set(stopwords.words('english'))\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "sp = en_core_web_lg.load()\n",
    "r = Rake()\n",
    "model1 = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41231d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    temp = \"\"\n",
    "    for i in text.split():\n",
    "        try:\n",
    "            temp += contraction[i]+' '\n",
    "        except:\n",
    "            temp += i+' '\n",
    "    text = temp.strip()\n",
    "    text = text.lower().translate(remove_punctuation_map)\n",
    "    text = re.sub(\"[^a-zA-Z#]\", \" \", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"!\", \"!\", text)\n",
    "    text = re.sub(r\"\\/\", \"\", text)\n",
    "    text = re.sub(r\"'\", \"\", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \":\", text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def stopwordremoval(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [i for i in text if i not in stopw]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f999e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"ans_ml.txt\", \"r\") as f:\n",
    "#     test_ans_ml = f.read().strip()\n",
    "# with open(\"ans_mit.txt\", \"r\") as f:\n",
    "#     test_ans_mit = f.read().strip()\n",
    "\n",
    "key_ml = '''Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.'''\n",
    "key_mit = '''The mitochondria is a double membrane bound organelle found in most eukaryotic organisms . Some cells . In some multicellular organisms may however lack them. A number of unicellular organisms have transformed this into other structures they ae also known as powerhouse of the cell . They take in nutrients breaks them down and creates rich molecules for the energy cells . The biochemical processes of the cell are known as cellular respiration frith'''\n",
    "\n",
    "\n",
    "\n",
    "# test_ans_ml = '''Machine Learning is the use of mathematical models to enable machines to perform tasks without instructions.'''\n",
    "# test_ans_mit = '''Mitochondria is the power house of the cell.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4b0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_set(text, op):\n",
    "    key_tokenized_sentences = sent_tokenize(text)\n",
    "    key_tokenized_words = word_tokenize(text)\n",
    "    if op == \"token_sent\":\n",
    "        return key_tokenized_sentences\n",
    "    elif op == \"token_word\":\n",
    "        return key_tokenized_words\n",
    "    elif op == \"clean_sent\":\n",
    "        return [clean(i) for i in key_tokenized_sentences]\n",
    "    elif op == \"clean_word\":\n",
    "        return [clean(i) for i in key_tokenized_words]\n",
    "    elif op == \"lem_sent\":\n",
    "        key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "        return [\" \".join([lemmatizer.lemmatize(j) for j in i.split()]) for i in key_clean_sentences]\n",
    "    elif op == \"lem_word\":\n",
    "        key_clean_words = pp_set(text, \"clean_word\")\n",
    "        return [lemmatizer.lemmatize(i) for i in key_clean_words]\n",
    "    elif op == \"prep_sent\":\n",
    "        key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "        return [\" \".join([i for i in j.split() if i not in stopw]) for j in key_clean_sentences]\n",
    "    elif op == \"prep_word\":\n",
    "        key_preprocessed_sentences = pp_set(text, \"prep_sent\")\n",
    "        key_preprocessed_words = []\n",
    "        for i in key_preprocessed_sentences:\n",
    "            key_preprocessed_words.extend(word_tokenize(i))\n",
    "        return key_preprocessed_words\n",
    "    elif op == \"pp_lem_word\":\n",
    "        return [lemmatizer.lemmatize(i) for i in pp_set(text, \"prep_word\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434597b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, model, num_features, index2word_set):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e50fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_sim(key1, key2):\n",
    "    print(\"key1:\", key1)\n",
    "    print(\"key2:\", key2)\n",
    "    try:\n",
    "        sim = model.wv.n_similarity(key1, key2)\n",
    "    except:\n",
    "        vec1 = avg_sentence_vector(\n",
    "            pp_set(key1, \"pp_lem_word\"), model1, 300, model.index_to_key)\n",
    "        vec2 = avg_sentence_vector(\n",
    "            pp_set(key2, \"pp_lem_word\"), model1, 300, model.index_to_key)\n",
    "        sim = cosine_similarity(vec1, vec2)[0][0]\n",
    "    finally:\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09847c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdabc0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytextrank.base.BaseTextRankFactory at 0x1b1845daeb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spacy.load('en_core_web_lg')\n",
    "sp.add_pipe('sentencizer')\n",
    "sp.add_pipe(\"textrank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97692eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    # Effectiveness : tokenized > lemmatized > clean\n",
    "    r.extract_keywords_from_sentences(pp_set(text, \"lem_sent\"))\n",
    "    rake_keywords = r.get_ranked_phrases()\n",
    "#     sp.add_pipe(\"textrank\")\n",
    "    spdoc = sp(text)\n",
    "    ner_keywords = []\n",
    "    for ent in spdoc.ents:\n",
    "        ner_keywords.append(ent.text)\n",
    "    spdoc = sp(\" \".join(pp_set(text, \"clean_word\")))\n",
    "    pytr_keywords = []\n",
    "    for p in spdoc._.phrases:\n",
    "        for term in p.chunks:\n",
    "            if term.text not in pytr_keywords and term.text not in stopw:\n",
    "                x = term.text\n",
    "                pytr_keywords.append(x)\n",
    "\n",
    "    all_keywords = rake_keywords+pytr_keywords+ner_keywords\n",
    "    all_keywords = list(set(all_keywords))\n",
    "    sorted_keywords = list(all_keywords)\n",
    "    sorted_keywords.sort()\n",
    "    for i in range(len(sorted_keywords)):\n",
    "        sorted_keywords[i] = re.sub(r' +', ' ', sorted_keywords[i])\n",
    "\n",
    "    return sorted_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbac6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(sorted_keywords):\n",
    "    grouped_keys = []\n",
    "    for i in sorted_keywords:\n",
    "        if len(grouped_keys) == 0:\n",
    "            grouped_keys.append([i])\n",
    "            continue\n",
    "        else:\n",
    "            flag = False\n",
    "            for j in grouped_keys:\n",
    "                if i in j:\n",
    "                    flag = True\n",
    "                    break\n",
    "                temp1 = \" \".join([lemmatizer.lemmatize(t)\n",
    "                                  for t in stopwordremoval(i).split()])\n",
    "                for k in j:\n",
    "                    temp2 = \" \".join([lemmatizer.lemmatize(t)\n",
    "                                      for t in stopwordremoval(k).split()])\n",
    "                    short = min(temp1, temp2)\n",
    "                    long = max(temp1, temp2)\n",
    "                    if short in long:\n",
    "                        flag = True\n",
    "                        j.append(i)\n",
    "                        break\n",
    "                if flag == True:\n",
    "                    break\n",
    "            if flag == False:\n",
    "                grouped_keys.append([i])\n",
    "    temp = []\n",
    "    for i in grouped_keys:\n",
    "        k = sorted(i, key=len)\n",
    "        temp.append(k)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4d81964",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_keys = group(extract_keywords(key_mit))\n",
    "# grouped_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f37b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(grouped_keys):\n",
    "    for i in range(len(grouped_keys)):\n",
    "        grouped_keys[i] = list(set(grouped_keys[i]))\n",
    "        temp = list(grouped_keys[i])\n",
    "        process_set = [\" \".join([lemmatizer.lemmatize(\n",
    "            l) for l in stopwordremoval(j).split()]) for j in grouped_keys[i]]\n",
    "        process_set = list(set(process_set))\n",
    "        for temp_key1 in grouped_keys[i]:\n",
    "            x = \" \".join([lemmatizer.lemmatize(k)\n",
    "                          for k in stopwordremoval(temp_key1).split()])\n",
    "            if process_set.count(x) > 1:\n",
    "                temp.remove(temp_key1)\n",
    "        grouped_keys[i] = temp\n",
    "        grouped_keys[i] = sorted(grouped_keys[i])\n",
    "\n",
    "    for i in range(len(grouped_keys)):\n",
    "        temp = list(grouped_keys[i])\n",
    "        for j in range(len(grouped_keys[i])):\n",
    "            word = grouped_keys[i][j]\n",
    "            for k in temp:\n",
    "                if word in k and word != k:\n",
    "                    temp.remove(word)\n",
    "                    break\n",
    "        grouped_keys[i] = sorted(temp, key=len, reverse=True)\n",
    "    grouped_keys = [i for i in grouped_keys if len(i) > 0]\n",
    "    return grouped_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e68143",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_keys = remove_duplicates(grouped_keys)\n",
    "# grouped_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e179c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize(grouped_keys):\n",
    "    temp_keywords = []\n",
    "    final_keywords = []\n",
    "    for i in grouped_keys:\n",
    "        for j in i:\n",
    "            temp_keywords.append(j)\n",
    "\n",
    "    temp_keywords = remove_duplicates(group(temp_keywords))\n",
    "\n",
    "    for i in temp_keywords:\n",
    "        for j in i:\n",
    "            final_keywords.append(j)\n",
    "    return final_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa06bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_keywords = finalize(grouped_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c56b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionarize(final_keywords, text):\n",
    "    answer_key = dict()\n",
    "    sentences = pp_set(text, \"token_sent\")\n",
    "    for i in sentences:\n",
    "        answer_key[i] = list()\n",
    "    temp = list(final_keywords)\n",
    "    for i in range(len(temp)):\n",
    "        key = \" \".join(pp_set(temp[i], \"token_word\"))\n",
    "        for j in answer_key:\n",
    "            x = j.strip().lower()\n",
    "            if key in x:\n",
    "                answer_key[j].append(key)\n",
    "                final_keywords.remove(temp[i])\n",
    "                break\n",
    "    return answer_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12679794",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key = dictionarize(final_keywords, key_mit)\n",
    "# answer_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c25d0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(answer_key):\n",
    "    vector_keys = []\n",
    "    vector_sent = []\n",
    "    for i in list(answer_key.keys()):\n",
    "        vector_sent.append(avg_sentence_vector(\n",
    "            pp_set(i, \"token_word\"), model1, 300, model1.index_to_key))\n",
    "        temp = []\n",
    "        for j in list(answer_key[i]):\n",
    "            temp.append(avg_sentence_vector(\n",
    "                pp_set(j, \"token_word\"), model1, 300, model1.index_to_key))\n",
    "        vector_keys.append(temp)\n",
    "\n",
    "    return vector_sent, vector_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abc5cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kw = extract_keywords(test_ans_mit)\n",
    "# final_kw = finalize(remove_duplicates(group(kw)))\n",
    "# answer_test = dictionarize(final_kw, test_ans_mit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58452ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_final = remove_duplicates(group(final_keywords))\n",
    "new_final = []\n",
    "for i in group_final:\n",
    "    if len(i) > 1:\n",
    "        for j in i:\n",
    "            for k in i:\n",
    "                if j != k and j in k:\n",
    "                    new_final.append(j)\n",
    "    else:\n",
    "        new_final.append(i[-1])\n",
    "# new_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cdbb923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "marks = 0\n",
    "for i in answer_key:\n",
    "    marks += len(answer_key[i])\n",
    "print(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69e94e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(key, test):\n",
    "    vec_key_sent, vec_key_keys = vectorize_text(key)\n",
    "    vec_test_sent, vec_test_keys = vectorize_text(test)\n",
    "    sum = 0\n",
    "    sims = dict()\n",
    "    for i in range(len(vec_test_sent)):\n",
    "        sims[i] = []\n",
    "        for j in range(len(vec_key_sent)):\n",
    "            sim = cosine_similarity(vec_test_sent[i].reshape(\n",
    "                1, -1), vec_key_sent[j].reshape(1, -1))\n",
    "            if sim > 0.7:\n",
    "                sims[i].append(j)\n",
    "\n",
    "    count = 0\n",
    "    for keyidx in sims:\n",
    "        ans_kw = vec_test_keys[keyidx]\n",
    "        key_kw = []\n",
    "        checked = []\n",
    "        for i in sims[keyidx]:\n",
    "            key_kw.extend(vec_key_keys[i])\n",
    "\n",
    "        for akw in ans_kw:\n",
    "            max_sim = -1\n",
    "            max_kkw = []\n",
    "            for kkw in key_kw:\n",
    "                if kkw in checked:\n",
    "                    continue\n",
    "                sim = cosine_similarity(kkw, akw)[0][0]\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    max_akw = kkw\n",
    "            if sim > 0.9:\n",
    "                sum += 1\n",
    "            else:\n",
    "                sum += max_sim\n",
    "            checked.append(max_kkw)\n",
    "    return sum, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bae5b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(test_ans_mit):\n",
    "    kw = extract_keywords(test_ans_mit)\n",
    "    final_kw = finalize(remove_duplicates(group(kw)))\n",
    "    answer_test = dictionarize(final_kw, test_ans_mit)\n",
    "    test_score, kw_match = score(answer_key, answer_test)\n",
    "    print(test_score)\n",
    "    test_score = test_score/marks*100\n",
    "    if (test_score % 1) > 0.5:\n",
    "        rem = 1\n",
    "    else:\n",
    "        rem = 0\n",
    "    final_score = int(test_score)+rem\n",
    "    #print(final_score, \"/\", 100, sep='')\n",
    "#     if(final_score<=20):\n",
    "#         print(\"Content is not relevant to the question\")\n",
    "#     elif(final_score<=50 ):\n",
    "#         print(\"The answer is missing few key points\")\n",
    "#     elif(final_score<=80):\n",
    "#         print(\"The answer can be improved\")\n",
    "#     elif(final_score<=100):\n",
    "#         print(\"Keep it up!\")\n",
    "    return final_score\n",
    "#print(result(answer_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ee07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [26/Feb/2023 01:05:46] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Feb/2023 01:05:46] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [26/Feb/2023 01:05:59] \"POST /ans HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from flask import Flask, request,redirect,url_for, render_template\n",
    "app=Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "# @app.route(\"/\",methods=[\"POST\",\"GET\"])\n",
    "# def index():\n",
    "#     if request.method==\"POST\":\n",
    "#         input1=request.form[\"nm\"]        \n",
    "#         return redirect(url_for(\"ans\",text=input1))\n",
    "#     else:\n",
    "#         return render_template(\"index.html\")\n",
    "    \n",
    "\n",
    "@app.route(\"/ans\",methods=[\"POST\",\"GET\"])\n",
    "def ans():\n",
    "    if request.method==\"POST\":\n",
    "        input1=request.form[\"nm1\"]\n",
    "        input2=request.form[\"nm2\"]\n",
    "    def first(text):\n",
    "        grouped_keys = group(extract_keywords(key_mit))\n",
    "        final_keywords = finalize(grouped_keys)\n",
    "        answer_key = dictionarize(final_keywords, key_mit)\n",
    "        group_final = remove_duplicates(group(final_keywords))\n",
    "        new_final = []\n",
    "        for i in group_final:\n",
    "            if len(i) > 1:\n",
    "                for j in i:\n",
    "                    for k in i:\n",
    "                        if j != k and j in k:\n",
    "                            new_final.append(j)\n",
    "            else:\n",
    "                new_final.append(i[-1])\n",
    "        marks = 0\n",
    "        for i in answer_key:\n",
    "            marks += len(answer_key[i])\n",
    "#print(marks)\n",
    "\n",
    "        kw = extract_keywords(text)\n",
    "        final_kw = finalize(remove_duplicates(group(kw)))\n",
    "        answer_test = dictionarize(final_kw, text)\n",
    "        test_score, kw_match = score(answer_key, answer_test)\n",
    "    #print(test_score)\n",
    "        test_score = test_score/marks*100\n",
    "        if (test_score % 1) > 0.5:\n",
    "            rem = 1\n",
    "        else:\n",
    "            rem = 0\n",
    "        resu = int(test_score)+rem\n",
    "        return resu\n",
    "    def second(text):\n",
    "        grouped_keys = group(extract_keywords(key_ml))\n",
    "        final_keywords = finalize(grouped_keys)\n",
    "        answer_key = dictionarize(final_keywords, key_ml)\n",
    "        group_final = remove_duplicates(group(final_keywords))\n",
    "        new_final = []\n",
    "        for i in group_final:\n",
    "            if len(i) > 1:\n",
    "                for j in i:\n",
    "                    for k in i:\n",
    "                        if j != k and j in k:\n",
    "                            new_final.append(j)\n",
    "            else:\n",
    "                new_final.append(i[-1])\n",
    "        marks = 0\n",
    "        for i in answer_key:\n",
    "            marks += len(answer_key[i])\n",
    "\n",
    "        kw = extract_keywords(text)\n",
    "        final_kw = finalize(remove_duplicates(group(kw)))\n",
    "        answer_test = dictionarize(final_kw, text)\n",
    "        test_score, kw_match = score(answer_key, answer_test)\n",
    "    #print(test_score)\n",
    "        test_score = test_score/marks*100\n",
    "        if (test_score % 1) > 0.5:\n",
    "            rem = 1\n",
    "        else:\n",
    "            rem = 0\n",
    "        resu = int(test_score)+rem\n",
    "        return resu\n",
    "    mark1=first(input1)\n",
    "    feed1=\"\"\n",
    "    if(mark1<=20):\n",
    "        feed1+=\"Content is not relevant to the question.\"\n",
    "    elif(mark1<=50 ):\n",
    "        feed1+=\"The answer is missing few key points\" \n",
    "    elif(mark1<=80):\n",
    "        feed1+=\"The answer can be improved.\" \n",
    "    elif(mark1<=100):\n",
    "        feed1+=\"Keep it up!\"\n",
    "    mark2=second(input2)\n",
    "    feed=\" The answer can be improved\"\n",
    "#     if(mark2<=20):\n",
    "#         feed+=\"Content is not relevant to the question.\"\n",
    "#         return \n",
    "#     elif(mark2<=50 ):\n",
    "#         feed+=\"The answer is missing few key points\" \n",
    "#         return \n",
    "#     elif(mark1<=80):\n",
    "#         feed+=\"The answer can be improved.\" \n",
    "#         return \n",
    "#     else:\n",
    "#         feed+=\"Keep it up!\"\n",
    "    return render_template(\"result.html\",mark1=mark1,feed1=feed1,mark2=mark2,feed=feed)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# @app.route(\"/<text>\")\n",
    "# def ans(text):\n",
    "#     #return text\n",
    "#        # resu = result(text)\n",
    "#     def first()\n",
    "#         grouped_keys = group(extract_keywords(key_mit))\n",
    "#         answer_key = dictionarize(final_keywords, key_mit)\n",
    "\n",
    "#         kw = extract_keywords(text)\n",
    "#         final_kw = finalize(remove_duplicates(group(kw)))\n",
    "#         answer_test = dictionarize(final_kw, text)\n",
    "#         test_score, kw_match = score(answer_key, answer_test)\n",
    "#     #print(test_score)\n",
    "#         test_score = test_score/marks*100\n",
    "#         if (test_score % 1) > 0.5:\n",
    "#             rem = 1\n",
    "#         else:\n",
    "#             rem = 0\n",
    "#         resu = int(test_score)+rem\n",
    "#         return res\n",
    "\n",
    "    \n",
    "#     feed=\"\"\n",
    "#     if(resu<=20):\n",
    "#         feed+=\"Content is not relevant to the question.\"\n",
    "#     elif(resu<=50 ):\n",
    "#         feed+=\"The answer is missing few key points\" \n",
    "#     elif(resu<=80):\n",
    "#         feed+=\"The answer can be improved.\" \n",
    "#     elif(resu<=100):\n",
    "#         feed+=\"Keep it up!\"\n",
    " \n",
    "#     return render_template(\"result.html\",resu=resu,feed=feed)\n",
    "#             #return f\"<h3>{resu}<h3>\"\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 9000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabf96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
